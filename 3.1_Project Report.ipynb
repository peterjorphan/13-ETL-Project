{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract\n",
    "We found great information on basketball players and their statics for the past 50 years. NBA players' salary\n",
    "information came from http://www.espn.com/nba/salaries. This website had multipe pages and tables that required us\n",
    "to scrape the information. We looped through each page first and then looped through each year in the dropdown list until complete. All data was imported and transformed into a working csv.\n",
    "\n",
    "\n",
    "All other files were basic csv files. Complete player data was (player_data.csv) from  https://www.kaggle.com/drgilermo/nba-players-stats.\n",
    "Basic player data like height and weight was also a csv file (Players.csv) from https://www.kaggle.com/drgilermo/nba-players-stats. Season statics was also a csv (Seasons_Stats.csv)\n",
    "taken from https://www.kaggle.com/drgilermo/nba-players-stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "### Salary (salary_raw.csv)\n",
    "- Drop lines that have all null values\n",
    "- Remove extra lines that came over with scrapping\n",
    "- Separate name from position and store in different columns\n",
    "- Create a name key by removing special characters and spaces, and converting to lower case. This is to be used as a foreign key for joinin with other tables\n",
    "- Changed season from YYYY-YYYY format to only YYYY using the 2nd part to be consistent with the season notation in the other tables\n",
    "- Name index to be used as primary key\n",
    "- Select the columns that have unique value in the database and reorder them *\n",
    "\n",
    "### Players (Players.csv)\n",
    "- Create a name key by removing special characters and spaces, and converting to lower case. After removal of duplicate lines this is a unique value and it was set up as the index in order to be the primary key of the table\n",
    "- Drop duplicate lines\n",
    "- Drop rows with all null values\n",
    "- Select the columns that have unique value in the database and reorder them *. Rename as needed.\n",
    "\n",
    "### Player Data (player_data.csv)\n",
    "- Create a name key by removing special characters and spaces, and converting to lower case. This is to be used as a foreign key for joinin with other tables\n",
    "- Name index to be used as primary key\n",
    "- Convert birthday field to a ISO standard date type\n",
    "- Select the columns that have unique value in the database and reorder them *\n",
    "\n",
    "### Season Statistics (Seasons_Stats.csv)\n",
    "- Drop columns with all null values and rows with any null values\n",
    "- Create a name key by removing special characters and spaces, and converting to lower case. This is to be used as a foreign key for joinin with other tables\n",
    "- Name index to be used as primary key\n",
    "- Renamed all column names to replace characters not compatible with MySQL\n",
    "- Select the columns that have unique value in the database and reorder them *\n",
    "\n",
    "\n",
    "\"*\" In selecting the columns for all the above tables, an effort was made to remove any redundancy, by choosing only one source for the same type of information. For example, height in cm was selected, while height in ft-in in another table was dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "We chose to load our datasets into MySQl because all our data were structured in a way to easily create relationships consistent with relational databases.\n",
    "\n",
    "With well organized, transformed and cleaned datasets we were able to easily pull the dataframes into previously created tables. We first created the database called basketball_db in MySQL Worbench.\n",
    "Then we created the four tables (salary,players, player_data, player_stats) that would house all four dataframes (salary_df, players_df, player_data_df, stats_df). Each table received column names that mirrored the column names on each dataframe, and were given data types consisent with the values in those DataFrame columns. \n",
    "\n",
    "After creating the above schema we loaded the selected DataFrames onto their respective tables. Any errors observed during this process made us go back to the transformation stage for more chagnes to our DataFrames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
